{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if config exists\n",
    "try:\n",
    "    config\n",
    "except NameError:\n",
    "    config_exists = False\n",
    "else:\n",
    "    config_exists = True\n",
    "\n",
    "# make config if it does not exist already (e.g. passed in by papermill)\n",
    "if not(config_exists):\n",
    "    # set up some config for the experiment run\n",
    "    config = {\n",
    "        \"data_path\" : \"C:/Users/nico_/Desktop/ITBA/TFI/github/df_procesado/trollers_fe_recortado_v2.csv\",\n",
    "    }\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing core libraries\n",
    "#https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "#https://www.kaggle.com/code/lucamassaron/tutorial-bayesian-optimization-with-lightgbm/notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "from functools import partial\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Classifiers\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, DeltaYStopper\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Data processing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "df = pd.read_csv(config['data_path'], header=\"infer\", sep=\",\",error_bad_lines=False, engine ='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "['mmsi', 'timestamp', 'distance_from_shore', 'distance_from_port',\n",
    "       'speed', 'course', 'lat', 'lon', 'is_fishing', 'source', 'datetime',\n",
    "       'timediff', 'sun_state', 'distance', 'S0', 'A0', 'J0', 'C0', 'Vavg',\n",
    "       'Delta_S', 'Cavg', 'Delta_C', 'speed_lag_1', 'speed_lag_2',\n",
    "       'speed_lag_3', 'speed_lag_4', 'speed_lag_5', 'speed_lag_6',\n",
    "       'speed_lag_7', 'course_lag_1', 'course_lag_2', 'course_lag_3',\n",
    "       'course_lag_4', 'course_lag_5', 'course_lag_6', 'course_lag_7',\n",
    "       'S0_lag_1', 'S0_lag_2', 'S0_lag_3', 'S0_lag_4', 'S0_lag_5', 'S0_lag_6',\n",
    "       'S0_lag_7', 'A0_lag_1', 'A0_lag_2', 'A0_lag_3', 'A0_lag_4', 'A0_lag_5',\n",
    "       'A0_lag_6', 'A0_lag_7', 'J0_lag_1', 'J0_lag_2', 'J0_lag_3', 'J0_lag_4',\n",
    "       'J0_lag_5', 'J0_lag_6', 'J0_lag_7', 'C0_lag_1', 'C0_lag_2', 'C0_lag_3',\n",
    "       'C0_lag_4', 'C0_lag_5', 'C0_lag_6', 'C0_lag_7', 'Delta_C_lag_1',\n",
    "       'Delta_C_lag_2', 'Delta_C_lag_3', 'Delta_C_lag_4', 'Delta_C_lag_5',\n",
    "       'Delta_C_lag_6', 'Delta_C_lag_7', 'Delta_S_lag_1', 'Delta_S_lag_2',\n",
    "       'Delta_S_lag_3', 'Delta_S_lag_4', 'Delta_S_lag_5', 'Delta_S_lag_6',\n",
    "       'Delta_S_lag_7', 'Vavg_lag_1', 'Vavg_lag_2', 'Vavg_lag_3', 'Vavg_lag_4',\n",
    "       'Vavg_lag_5', 'Vavg_lag_6', 'Vavg_lag_7']\n",
    "'''\n",
    "df = df.drop(['mmsi', 'source','timestamp','source','datetime','distance_from_shore','distance_from_port'], axis = 1)\n",
    "lagged_columns =['distance', 'S0', 'A0', 'J0', 'C0', 'Vavg',\n",
    "       'Delta_S', 'Cavg', 'Delta_C', 'speed_lag_1', 'speed_lag_2',\n",
    "       'speed_lag_3', 'speed_lag_4', 'speed_lag_5', 'speed_lag_6',\n",
    "       'speed_lag_7', 'course_lag_1', 'course_lag_2', 'course_lag_3',\n",
    "       'course_lag_4', 'course_lag_5', 'course_lag_6', 'course_lag_7',\n",
    "       'S0_lag_1', 'S0_lag_2', 'S0_lag_3', 'S0_lag_4', 'S0_lag_5', 'S0_lag_6',\n",
    "       'S0_lag_7', 'A0_lag_1', 'A0_lag_2', 'A0_lag_3', 'A0_lag_4', 'A0_lag_5',\n",
    "       'A0_lag_6', 'A0_lag_7', 'J0_lag_1', 'J0_lag_2', 'J0_lag_3', 'J0_lag_4',\n",
    "       'J0_lag_5', 'J0_lag_6', 'J0_lag_7', 'C0_lag_1', 'C0_lag_2', 'C0_lag_3',\n",
    "       'C0_lag_4', 'C0_lag_5', 'C0_lag_6', 'C0_lag_7', 'Delta_C_lag_1',\n",
    "       'Delta_C_lag_2', 'Delta_C_lag_3', 'Delta_C_lag_4', 'Delta_C_lag_5',\n",
    "       'Delta_C_lag_6', 'Delta_C_lag_7', 'Delta_S_lag_1', 'Delta_S_lag_2',\n",
    "       'Delta_S_lag_3', 'Delta_S_lag_4', 'Delta_S_lag_5', 'Delta_S_lag_6',\n",
    "       'Delta_S_lag_7', 'Vavg_lag_1', 'Vavg_lag_2', 'Vavg_lag_3', 'Vavg_lag_4',\n",
    "       'Vavg_lag_5', 'Vavg_lag_6', 'Vavg_lag_7']\n",
    "for column in lagged_columns:\n",
    "    df[column]=df[column].abs()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos los vectores de train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('is_fishing',axis=1)\n",
    "y = df['is_fishing']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=50)\n",
    "print(len(df[df['is_fishing']=='SI']))\n",
    "print(len(df[df['is_fishing']=='NO']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para evaluar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(title,y_test, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    display(Markdown(title))\n",
    "    cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['NO', 'SI'])\n",
    "    cmd_obj.plot()\n",
    "    cmd_obj.ax_.set(\n",
    "                    title='Matriz de confusi√≥n.', \n",
    "                    xlabel='Predict Values', \n",
    "                    ylabel='Actual Values')\n",
    "    plt.show()\n",
    "    _precision_score = precision_score(y_test, y_pred,pos_label='SI')\n",
    "    _recall_score = recall_score(y_test, y_pred, pos_label='SI')\n",
    "    _accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    _f1_score = f1_score(y_test, y_pred, pos_label='SI')\n",
    "\n",
    "    data = [[\"Precision\",\"Recall\",\"Accuracy\",\"F1\"],\n",
    "            [\"%.2f\" % _precision_score, \"%.2f\" % _recall_score, \"%.2f\" % _accuracy_score, \"%.2f\" % _f1_score ]]\n",
    "    table = tabulate.tabulate(data, tablefmt='html')\n",
    "    display(table)\n",
    "    return _precision_score, _recall_score, _accuracy_score, _f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  # pip install optuna\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [100]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.4),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 1023, step=10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100, step=2),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 100),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            #eval_metric=\"auc\",\n",
    "            early_stopping_rounds=100,\n",
    "            callbacks=[\n",
    "                #LightGBMPruningCallback(trial, \"auc\")\n",
    "                LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "\n",
    "#AUC -> maximixe\n",
    "#study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(objective=\"binary\", **study.best_params)\n",
    "model = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            #eval_metric=\"auc\",\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=0\n",
    "            \n",
    "        )\n",
    "y_pred = model.predict(X_test)\n",
    "lgbm_precision_score_auc, lgbm_recall_score_auc, lgbm_accuracy_score_auc, lgbm_f1_score_auc = evaluate_model('lgbm grid search', y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(model, max_num_features=30,importance_type ='gain',figsize=(20, 10) )\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8a79dfa68d3b59c718a5e00f96adc917a1b17d46c628831ab97d3d56be05add"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
